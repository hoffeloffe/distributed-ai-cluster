#!/usr/bin/env python3
"""
LIVE EXAMPLE: How the Distributed AI System Actually Works
Real code showing component interaction and data flow
"""

import asyncio
import json
import socket
import struct
import pickle
import time
import uuid
from typing import Dict, Any, Optional
import numpy as np

# Simulated components for demonstration
class MessageBroker:
    """Handles all inter-node communication"""

    def __init__(self, node_id: str, node_ip: str):
        self.node_id = node_id
        self.node_ip = node_ip
        self.connections: Dict[str, socket.socket] = {}

    def create_message(self, msg_type: str, target: str, payload: Dict) -> Dict:
        """Create a standardized message"""
        return {
            "message_id": str(uuid.uuid4()),
            "message_type": msg_type,
            "source_node": self.node_id,
            "target_node": target,
            "payload": payload,
            "timestamp": time.time()
        }

    def send_tcp_message(self, target_ip: str, message: Dict) -> bool:
        """Send message via TCP (reliable)"""
        try:
            # Get or create connection
            if target_ip not in self.connections:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.connect((target_ip, 8888))
                self.connections[target_ip] = sock

            # Serialize and send
            data = pickle.dumps(message)
            length = struct.pack('!I', len(data))

            self.connections[target_ip].send(length + data)
            return True

        except Exception as e:
            print(f"Failed to send TCP message: {e}")
            return False

class MasterNode:
    """Master node coordination logic"""

    def __init__(self, node_id: str):
        self.node_id = node_id
        self.message_broker = MessageBroker(node_id, "192.168.1.100")
        self.worker_nodes: Dict[str, Dict] = {}
        self.model_shards = {
            "shard_1": {"worker": "worker_1", "layers": [0, 1, 2]},
            "shard_2": {"worker": "worker_2", "layers": [3, 4, 5]},
            "shard_3": {"worker": "worker_3", "layers": [6, 7, 8]}
        }

    async def handle_worker_registration(self, worker_info: Dict):
        """Handle new worker joining the cluster"""
        worker_id = worker_info["worker_id"]
        worker_ip = worker_info["worker_ip"]

        print(f"ðŸ”— Registering worker {worker_id} at {worker_ip}")

        # Add to active workers
        self.worker_nodes[worker_id] = {
            "ip": worker_ip,
            "status": "active",
            "last_seen": time.time(),
            "capabilities": worker_info.get("capabilities", []),
            "current_load": 0
        }

        # Assign model shard
        shard_assignment = self._assign_model_shard(worker_id)
        if shard_assignment:
            await self._send_shard_assignment(worker_id, shard_assignment)

        print(f"âœ… Worker {worker_id} registered successfully")

    def _assign_model_shard(self, worker_id: str) -> Optional[Dict]:
        """Assign appropriate model shard to worker"""
        for shard_id, shard_info in self.model_shards.items():
            if shard_info["worker"] == worker_id:
                return {
                    "shard_id": shard_id,
                    "layers": shard_info["layers"],
                    "model_path": f"/models/shard_{shard_id}.h5"
                }
        return None

    async def _send_shard_assignment(self, worker_id: str, assignment: Dict):
        """Send model shard assignment to worker"""
        message = self.message_broker.create_message(
            "model_assignment",
            worker_id,
            assignment
        )

        worker_ip = self.worker_nodes[worker_id]["ip"]
        success = self.message_broker.send_tcp_message(worker_ip, message)

        if success:
            print(f"ðŸ“¦ Sent model shard assignment to {worker_id}")
        else:
            print(f"âŒ Failed to send shard assignment to {worker_id}")

    async def distribute_inference_request(self, input_data: np.ndarray) -> Dict:
        """Distribute AI inference across workers"""
        print(f"ðŸŽ¯ Distributing inference request to {len(self.worker_nodes)} workers")

        # Select best worker (round-robin for simplicity)
        active_workers = [w for w in self.worker_nodes.values() if w["status"] == "active"]
        if not active_workers:
            return {"error": "No active workers available"}

        selected_worker = active_workers[0]["ip"]

        # Send inference request
        request_message = self.message_broker.create_message(
            "inference_request",
            "worker",
            {
                "input_shape": input_data.shape,
                "input_data": input_data.tolist(),  # Convert for JSON
                "request_id": str(uuid.uuid4())
            }
        )

        success = self.message_broker.send_tcp_message(selected_worker, request_message)

        if success:
            print(f"ðŸš€ Inference request sent to {selected_worker}")
            return {"status": "distributed", "target_worker": selected_worker}
        else:
            return {"error": "Failed to send inference request"}

class WorkerNode:
    """Worker node inference logic"""

    def __init__(self, node_id: str, node_ip: str):
        self.node_id = node_id
        self.node_ip = node_ip
        self.message_broker = MessageBroker(node_id, node_ip)
        self.model_shard = None
        self.is_processing = False

    def load_model_shard(self, shard_info: Dict):
        """Load assigned model shard"""
        print(f"ðŸ’¾ Loading model shard: {shard_info['shard_id']}")

        # Simulate model loading
        self.model_shard = {
            "shard_id": shard_info["shard_id"],
            "layers": shard_info["layers"],
            "loaded": True
        }

        print(f"âœ… Model shard {shard_info['shard_id']} loaded successfully")

    async def handle_inference_request(self, request_data: Dict) -> Dict:
        """Process inference request"""
        print(f"ðŸ§  Processing inference request: {request_data['request_id']}")

        if not self.model_shard:
            return {"error": "No model shard loaded"}

        if self.is_processing:
            return {"error": "Worker is busy"}

        self.is_processing = True
        start_time = time.time()

        try:
            # Convert input data back to numpy array
            input_array = np.array(request_data["input_data"])

            # Simulate AI inference
            await asyncio.sleep(0.1)  # Simulate processing time

            # Generate mock results
            results = {
                "predictions": [
                    {"class": "cat", "confidence": 0.85},
                    {"class": "dog", "confidence": 0.12},
                    {"class": "bird", "confidence": 0.03}
                ],
                "processing_time": (time.time() - start_time) * 1000,
                "worker_id": self.node_id,
                "model_shard": self.model_shard["shard_id"]
            }

            print(f"âœ… Inference completed in {results['processing_time']:.2f}ms")
            return results

        finally:
            self.is_processing = False

class ClusterDemo:
    """Demonstrate the complete system working together"""

    def __init__(self):
        self.master = MasterNode("master-1")
        self.workers = {
            "worker-1": WorkerNode("worker-1", "192.168.1.101"),
            "worker-2": WorkerNode("worker-2", "192.168.1.102"),
            "worker-3": WorkerNode("worker-3", "192.168.1.103")
        }

    async def simulate_cluster_operation(self):
        """Simulate complete cluster operation"""
        print("ðŸš€ Starting Distributed AI Cluster Demo")
        print("=" * 50)

        # 1. Worker registration
        print("\nðŸ“‹ Phase 1: Worker Registration")
        for worker in self.workers.values():
            # Simulate worker registration message
            registration_msg = {
                "worker_id": worker.node_id,
                "worker_ip": worker.node_ip,
                "capabilities": ["tensorflow", "image_classification"],
                "hardware": {"cpu_cores": 4, "memory_gb": 8}
            }
            await self.master.handle_worker_registration(registration_msg)

        # 2. Model shard distribution
        print("\nðŸ“¦ Phase 2: Model Shard Distribution")
        for worker_id, worker in self.workers.items():
            shard_assignment = self.master._assign_model_shard(worker_id)
            if shard_assignment:
                worker.load_model_shard(shard_assignment)
                await self.master._send_shard_assignment(worker_id, shard_assignment)

        # 3. Inference request processing
        print("\nðŸŽ¯ Phase 3: Distributed Inference")
        test_input = np.random.random((1, 224, 224, 3))

        # Distribute request through master
        distribution_result = await self.master.distribute_inference_request(test_input)
        print(f"Distribution result: {distribution_result}")

        # Process on worker (simulate)
        if "target_worker" in distribution_result:
            target_ip = distribution_result["target_worker"]
            # Find worker by IP (in real system, this would be via network)
            for worker in self.workers.values():
                if worker.node_ip == target_ip:
                    # Simulate inference processing
                    inference_result = await worker.handle_inference_request({
                        "request_id": "demo-request-1",
                        "input_data": test_input.tolist()
                    })
                    print(f"Worker result: {inference_result}")
                    break

        # 4. Show cluster status
        print("\nðŸ“Š Phase 4: Cluster Status")
        print(f"Active workers: {len(self.master.worker_nodes)}")
        print(f"Model shards distributed: {len(self.master.model_shards)}")

        for worker_id, worker_info in self.master.worker_nodes.items():
            print(f"  {worker_id}: {worker_info['status']} (Load: {worker_info['current_load']})")

        print("\nâœ… Distributed AI Cluster Demo Complete!")

# Real network communication example
class RealNetworkExample:
    """Shows actual network communication between nodes"""

    @staticmethod
    def demonstrate_message_passing():
        """Show how messages actually travel between nodes"""
        print("\nðŸ”— Network Communication Example:")
        print("-" * 40)

        # Master creates and sends heartbeat
        master_broker = MessageBroker("master", "192.168.1.100")

        heartbeat_msg = master_broker.create_message(
            "heartbeat",
            "worker-1",
            {
                "cluster_status": "healthy",
                "active_workers": 3,
                "total_requests": 150
            }
        )

        print("1. Master creates heartbeat message:")
        print(f"   Message ID: {heartbeat_msg['message_id'][:8]}...")
        print(f"   Type: {heartbeat_msg['message_type']}")
        print(f"   Target: {heartbeat_msg['target_node']}")

        # Worker receives and processes message
        print("\n2. Worker receives and processes message:")
        print("   - Validates message signature")
        print("   - Updates local cluster status")
        print("   - Responds with worker metrics")

        # Simulate message serialization
        serialized = pickle.dumps(heartbeat_msg)
        print(f"\n3. Message serialization: {len(serialized)} bytes")

        # Network transmission simulation
        print("\n4. Network transmission:")
        print("   TCP Connection: 192.168.1.100:8888 â†’ 192.168.1.101:8888")
        print("   Length prefix: 4 bytes")
        print(f"   Payload: {len(serialized)} bytes")
        print("   Total packet size: 4 +", len(serialized), "bytes")

# Run the complete demonstration
async def main():
    """Run complete system demonstration"""

    print("ðŸ¤– DISTRIBUTED AI CLUSTER - HOW IT WORKS")
    print("=" * 60)

    # 1. System overview
    demo = ClusterDemo()
    await demo.simulate_cluster_operation()

    # 2. Technical deep dive
    RealNetworkExample.demonstrate_message_passing()

    print("\nðŸŽ¯ KEY TAKEAWAYS:")
    print("â€¢ Master coordinates workers via TCP messages")
    print("â€¢ Model shards are distributed for parallel processing")
    print("â€¢ Heartbeat protocol monitors system health")
    print("â€¢ Load balancing ensures optimal resource usage")
    print("â€¢ All communication is asynchronous and fault-tolerant")

if __name__ == "__main__":
    asyncio.run(main())
